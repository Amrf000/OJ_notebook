---
layout: post
title: 使用 Python 和 Pytest 测试并行化
category: python
tags: [pytest]
---
并行化:
https://github.com/browsertron/pytest-parallel
https://stackoverflow.com/questions/45733763/pytest-run-tests-parallel
https://www.tutorialspoint.com/pytest/pytest_run_tests_in_parallel.htm
默认情况下，pytest 按顺序运行测试。在实际场景中，一个测试套件将有许多测试文件，每个文件将有一堆测试。这将导致很长的执行时间。为了克服这个问题，pytest 为我们提供了一个并行运行测试的选项。

为此，我们需要先安装 pytest-xdist 插件。

通过运行以下命令安装 pytest-xdist -

pip install pytest-xdist
现在，我们可以使用语法pytest -n <num>运行测试

pytest -n 3
-n <num> 使用多个 worker 运行测试，这里是 3。

当只有几个测试要运行时，我们不会有太大的时差。但是，当测试套件很大时这很重要。
================================================================================================
https://stackoverflow.com/questions/56072381/how-can-i-make-the-output-less-verbose
https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a
================================================================================================
https://stackoverflow.com/questions/27884404/printing-test-execution-times-and-pinning-down-slow-tests-with-py-test
https://github.com/scoutapp/scout_apm_python
https://www.libhunt.com/l/python/t/profiler
[在 Python 中优化性能](https://scoutapm.com/blog/identifying-bottlenecks-and-optimizing-performance-in-a-python-codebase)
识别 Python 代码库中的瓶颈并优化性能
======================

![](https://en.gravatar.com/userimage/104272289/7b4057cbcec59a7c25ef7171bd0421b0.jpg) [萨特维克坎萨尔](/blog/author/satwik-kansal)  2019 年 7 月 8 日

![](/md_blog/public/assets/2021-08-30/optimize_python_code.png) 

![脸书分享按钮](https://platform-cdn.sharethis.com/img/facebook.svg)

![推特分享按钮](https://platform-cdn.sharethis.com/img/twitter.svg)

![链接分享按钮](https://platform-cdn.sharethis.com/img/linkedin.svg)

![reddit 分享按钮](https://platform-cdn.sharethis.com/img/reddit.svg)

[工程](/blog/categories/engineering) [Python](/blog/categories/python)

在 Python 中优化性能
--------------

在这篇文章中，我们将介绍可用于识别 Python 代码库中的性能瓶颈并对其进行优化的各种技术。

无论如何，“优化代码”是什么意思？
-----------------

术语“优化”可以适用于广泛的指标级别。但最感兴趣的两个一般指标是：CPU 性能（执行时间）和内存占用。对于这篇文章，您可以将优化的代码视为能够运行得更快或使用更少内存或两者兼而有之的代码。

什么时候优化？
-------

没有硬性规定。但一般来说，当你确定代码中的业务逻辑是正确的并且不会很快改变时，你应该尝试优化。

> “首先让它工作。然后让它正确。然后让它快起来。” ~ 肯特贝克

否则，花在过早优化上的努力可能是徒劳的。

除非您正在开发性能密集型产品或将被其他可能是性能密集型项目使用的代码依赖项，否则优化代码的每个方面都可能是矫枉过正。对于大多数场景，80-20 原则（80% 的性能优势可能来自优化 20% 的代码）会更合适。根据唐纳德·克努斯教授的说法，

> _程序员浪费了大量时间来考虑或担心他们程序中非关键部分的速度，而这些对效率的尝试在考虑调试和维护时实际上会产生强烈的负面影响。我们应该忘记小效率，比如大约 97% 的时间：过早的优化是万恶之源。然而，我们不应该错过关键的 3% 的机会。_

在编写代码的优化版本之前，我们需要知道优化什么，“我应该把精力花在哪 3% 上？”，或者换句话说，我们需要找出瓶颈，

识别性能瓶颈
------

大多数时候我们会凭直觉做出猜测，但通常情况下，我们的猜测要么是错误的，要么只是大致正确。因此，始终建议借助工具来获得清晰明确的图片。测量程序不同部分使用的资源（CPU 使用率、RAM、I/O 等）的过程称为“分析”，有助于分析的工具通常称为“分析器”。分析器通常会收集统计信息，例如资源的使用频率以及谁在使用资源。例如，为分析执行时间而设计的分析器将测量代码的各个部分的执行频率和执行时间。当代码库变大并且您仍然希望保持效率时，使用分析机制变得必要。

Python 中的分析器
------------

Python 中的分析器的世界非常大，如果这是您第一次决定分析某些东西，您可能会被拥有的选项数量所淹没。但好消息是一些分析器最适合特定情况，了解这一点可以帮助您在决定使用哪个分析器时避免很多压力，这正是我们将在接下来的几节中介绍的内容。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#different-kinds-of-profilers)不同类型的分析器

您可以根据多个标准在 python 中分离分析器，

#### 确定性和统计分析

分析器通过监视程序执行期间发生的事件（函数调用、返回、异常等）来收集数据和统计信息。大致有两种方法可以解决这个问题，要么分析器将监视所有事件，要么在时间间隔后采样以收集该信息。前者称为确定性分析，后者称为统计分析。我们在这两种方法中所做的权衡是在开销和准确性之间进行的。您可能会发现术语确定性和统计分析器与“跟踪”和“采样”分析器可互换使用，但从技术上讲，它们并不相同。跟踪只是意味着记录整个调用图，而采样只是意味着每隔一段时间进行探测。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#the-level-at-which-resources-are-measured)衡量资源的级别

一些分析器会在模块级别测量资源使用情况，一些能够在功能级别进行测量，有些可以在线路级别进行测量。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#profile-visualizers)配置文件可视化工具

有一些工具本身不是分析器，但它们提供了一个界面来对其他流行的分析器收集的数据/生成的报告进行分析。

在 Python 中分析 CPU 性能（和执行时间）
--------------------------

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#useful-cpu-performance-profilers-in-python)python中有用的CPU性能分析器

本节将引导您了解我们可用的各种选项。在可能的情况下，我们会将类似的分析器归为一组以进行简化。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#inbuilt-timing-modules)内置计时模块

计时模块（`time`module 和 the `timeit`）是基本模块，您可以使用它们来测量各种方法（一种算法测试）的执行时间。用法很简单。

导入时间
start\_time \= time.time()
\# 要评估的代码
end\_time \= time.time()
\# 时间以秒为单位
time\_taken \= end\_time \- start\_time

但是，有一些外部因素会影响`time_taken`。为了尽量减少这些外部因素（例如，机器负载、I/O）的影响，通常建议多次运行测试并选择最快的。

另一件要注意的事情是`time.time()`测量挂钟时间（代码完全执行所花费的总时间）而不是 CPU 时间（CPU 实际处理与代码相关的语句的总时间，不包括时间它在其他地方很忙）。要测量 CPU 时间，您可以`time.clock()`改用。

`timeit`模块旨在抽象出很多这样的复杂性，并提供一个简单的界面来测量执行时间。这是一个示例用法

\>> \> 进口timeit
 \>> \> timeit.timeit（语句\= ' “”。加入（some\_list）'，设置\= ' some\_list = \[ “让”， “加入”， “一些”， “字符串”\] '，编号\= 1000 ,计时器\= time.clock)
 0.00020100000000011775

上面的代码将运行该`setup`语句一次，然后返回运行主语句 ( `stmt`) 1000 次所花费的累计挂钟时间。

您还可以使用`timeit.repeat`多次运行相同的实验。并且要测量 CPU 时间而不是挂钟时间，您可以将计时器传递为`time.process_time`)。这是一个例子，

\>> \> timeit.repeat( stmt \= ' " ".join(some\_list) ' , setup \= ' some\_list = \["lets", "join", "some", "strings"\] ' , number \= 1000 , repeat \= 4 ,  计时器\= time.process\_time)
\[ 0.00019499999999972317，
  0.0001930000000003318，
  0.00019100000000005224，
  0.00019799999999969842 \]

此外，如果您使用的是 jupyter notebook，则可以使用[`%timeit`魔法函数](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)来获得类似（且格式良好）的结果。

#### profile 和 cProfile

内置的 profile 和 cProfile 模块提供确定性的分析。两者都有类似的接口，但配置文件是用纯 python 实现的，这增加了开销。

用法：

$ python -m profile somefile.py \* args
$ python -m cProfile somefile.py \* args

`*args`要传递给 python 文件的命令行参数在哪里（如果有）。

这将以以下格式在标准输出上打印结果。

       844321 function calls (711197 primitive calls) in 7.343 seconds
       Ordered by: standard name
     
       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        992    0.001    0.000    0.001    0.000 json_generator.py:49(is_interactive_statement)
    

您还可以使用该`-o`选项将结果保存到文件中，以便我们以后可以使用它们。

**结果中的不同列是什么意思？**

`ncalls`是调用次数，`tottime`是在给定函数中花费的时间，_不包括_调用子函数所花费的时间，`percall`是函数每次调用的时间（基本上`tottime`除以`ncalls`），`cumtime`是在函数中花费的累积时间，_包括_时间花费在对子函数的调用中，`percall`接下来是每次调用的累积时间，最后，`filename:lineno`提供对与这些统计信息相关的代码（在函数级别）的引用。

[这](https://pastebin.com/AWZXkzSU)和[这个](https://pastebin.com/bqzmUb8W)人的样本输出，当我试图一对夫妇我的脚本运行它们。如您所见，这些输出中有大量信息需要处理。如果您只对信息的一个子集感兴趣，您还可以将它们用作 Python 模块来分析代码的特定部分。

导入cProfile
p \= cProfile.Profile()
\# 启用分析
p.enbale()
\# 你想要分析的一些逻辑
\# 禁用分析
p.disable()
\# 打印统计信息
p.print\_stats()
\# 将统计数据转储到文件
p.dump\_stats( " results.prof " )

`profile`模块的接口略有不同，它也有一个[校准](https://docs.python.org/2/library/profile.html#calibration)方法，用于从结果中校正开销时间。如果您不扩展分析器而只想查看统计信息，则应始终使用 cProfile 而不是 profile，因为它的开销较小。

但是，很少有 cProfile 和 profile 模块对您没有帮助的情况，

*   基于事件循环的应用程序；cProfile 不了解 gevent 或任何基于 greenlet 的应用程序。
*   逐行分析；有时您会想知道调用耗时函数的行，有时您可能难以确定耗时函数中哪一行最慢。
*   缺乏可视化功能。不支持将统计数据导出为 callgrind 等格式。
*   Tricky 会做一些事情，比如测量 CPU 时间、分析多线程程序等等。默认情况下，CProfiler 仅在主线程（调用 Python 脚本的线程）上运行。如果您想分析从您的程序产生的其他线程，您可能需要使用[`threading.setprofile`](https://docs.python.org/3/library/threading.html#threading.setprofile) 线程模块的方法。

不过这里不用担心，还有其他工具可以补充 cProfile 或在某些情况下作为 cProfiler 的替代品。让我们接下来讨论它们。

#### pstats

pstats 模块可用于解析 profile 或 cProfile 模块生成的数据。解析后，您可以执行以下操作

*   根据通话次数、时间、累计时间等排序
*   结合来自多个来源的统计数据
*   从数据中剥离目录名称
*   查看有限的数据（基于像 top n、包含正则表达式等的标准）
*   查找函数的调用者和被调用者

这是典型的用法

导入pstats
\# 解析 cProfile
的结果s \= pstats.Stats( " results.prof " )
\# 添加统计信息
s.add( " more\_results.prof " )
\# 删除目录路径
s.strip\_dirs()
\# 根据累计时间排序
s.sort\_stats( " cumulative " )
\# 打印前 'n' 个统计信息
n \=  3
s.print\_stats(n)
函数名称\=  " foo "
\# 打印 foo 的调用者
stats.print\_callers( ' \\( {} ' .format(function\_name))
\# foo 
stats.print\_callers( ' \\( {} ' .format(function\_name))的pring被调用者

#### line\_profiler

[line\_profiler](https://github.com/rkern/line_profiler)执行逐行分析，收集有关在被分析的每一行代码中花费的时间的统计信息。

**用法**

$ pip install line\_profiler

#你的python程序
\# 使用装饰器
@profile 
def  some\_func ( \* args , \*\* kwargs )
     ...

\# kernprof 是一个方便的脚本来运行不同的分析器；它带有 line\_profiler 
\# https://github.com/rkern/line\_profiler/blob/master/kernprof.py
$ kernprof -l -v python\_script.py
\# 这将生成一个名为 python\_script.py.lprof 的二进制文件
\# 你可以通过
$ python -m line\_profiler python\_script.py.lprof来查看这个文件

这是一个示例输出（[此处为](https://pastebin.com/NKq6PWWE)完整输出）

    Timer unit: 1e-06 s
    
    Total time: 0.000825 s
    File: json_generator.py
    Function: generate_code_block at line 26
    
    Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        26                                           @profile
        27                                           def generate_code_block(statements, output):
        28                                               global sequence_num
        29                                               result = {
        30       278        109.0      0.4     13.2          "type": "code",
        31       278        143.0      0.5     17.3          "sequence_num": sequence_num,
        32       278        109.0      0.4     13.2          "statements": statements,
        33       278        220.0      0.8     26.7          "output": output
        34                                               }
        35       278        130.0      0.5     15.8      sequence_num += 1
        36       278        114.0      0.4     13.8      return result
    
    Total time: 0.014138 s
    File: json_generator.py
    Function: parse_example_parts at line 55
    
    Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        55                                           @profile
        56                                           def parse_example_parts(lines, example_title_line):
        57                                               parts = {
        58        57         38.0      0.7      0.3          "build_up": [],
    

列名是一种不言自明的。Hits 是该行在运行过程中被执行的次数，“% time”是该行花费的时间比例。（每个函数的总和为 100）。

#### 雅皮

Yappi (Yet Another Python Profiler) 是一个分析器，它试图改进 cProfile 的一些缺乏的功能。雅皮可以，

*   开箱即用地分析多线程 python 应用程序。它还提供获取每线程函数统计信息的功能。
*   轻松测量 CPU 时间。
*   将结果导出为 callgrind 格式以及 pstat 格式。

**用法**

$ pip 安装 yappi

进口亚皮
\# 默认时钟设置为 CPU，但您可以切换到挂钟
\# yappi.set\_clock\_type("wall")
yappi.start()
\# 在这里调用你的函数或执行
some\_func()
\# 获取所有统计信息
yappi.get\_func\_stats().print\_all()

这是示例输出（[此处为](https://pastebin.com/XdbKU48w)完整输出）。

名称 ncall tsub ttot tavg
../lib/python3.6/pprint.py:47 pprint 2 0.000050 5.181650 2.590825
..pprint.py:138 PrettyPrinter.pprint 2 0.000043 5.181576 2.590788
..print.py:154 PrettyPrinter.\_format 101.. 0.234201 5.181516 0.000508
...py:180 PrettyPrinter.\_pprint\_dict 148.. 0.047156 4.244369 0.002868
..3 PrettyPrinter.\_format\_dict\_items 148.. 0.149224 4.232629 0.002860
...py:207 PrettyPrinter.\_pprint\_list 999/2 0.016574 4.169317 0.004173
..py:350 PrettyPrinter.\_format\_items 999/2 0.086727 4.169248 0.004173
..nerator.py:226 convert\_to\_notebook 1 0.001365 3.705809 3.705809
../pprint.py:391 PrettyPrinter.\_repr 15608 0.192905 3.533774 0.000226
..pprint.py:400 PrettyPrinter.format 15608 0.120687 3.277539 0.000210
..python3.6/pprint.py:490 \_safe\_repr 931.. 1.554210 3.156852 0.000034
..ython3.6/json/\_\_init\_\_.py:120 转储 1 0.124881 0.937511 0.937511
...6/json/encoder.py:412 \_iterencode 19093 0.116629 0.749296 0.000039
..on/encoder.py:333 \_iterencode\_dict 488.. 0.329734 0.632647 0.000013
..t.py:244 PrettyPrinter.\_pprint\_str 1020 0.277267 0.576017 0.000565
..python3.6/pprint.py:94 \_safe\_tuple 30235 0.352826 0.559835 0.000019
..on/encoder.py:277 \_iterencode\_list 297.. 0.201149 0.515704 0.000017
...6/pprint.py:84 \_safe\_key.\_\_init\_\_ 60470 0.207009 0.207009 0.000003
..n3.6/pprint.py:87 \_safe\_key.\_\_lt\_\_ 32160 0.114377 0.114377 0.000004
..enerator.py:60 parse\_example\_parts 57 0.043338 0.104288 0.001830
..\_generator.py:173 convert\_to\_cells 112 0.004576 0.044347 0.000396
..62 inspect\_and\_sanitize\_code\_lines 278 0.011167 0.037915 0.000136

就像 cProfiler 一样，指标是在函数级别收集的。`tsub`是在函数中花费的总时间，不包括子调用，`ttot`是包括它们的总时间，`ncall`是调用次数，`tavg`是每次调用时间（`ttot`除以`ncall`）。

#### 热点和冷点

hotshot 也是一个内置的分析模块。hotshot 和 cProfile（或配置文件）之间的区别在于，hotshot 不会像 cProfile 那样干扰。hotshot 背后的想法是通过在最后进行所有数据处理来最小化分析开销。但是，该模块不再维护，python 文档说它可能会在未来版本中删除。

hostshot的界面和cProfile类似；你可以在[这里](https://docs.python.org/2/library/hotshot.html)阅读文档。

Coldshot 是对 hotshot 的改进；主要功能是能够在像 RunSnakeRun 这样的工具中可视化结果（稍后讨论）。

#### vmprof-python，pyinstrument

vmprof-python 是 Python 的统计线分析器。由于统计性质，vmprof-python 的开销很低。

**用法**

$ pip 安装 vmprof
$ python -m vmprof --lines -o results.dat somefile.py
$ vmprofshow --lines 结果.dat

您可以省略`--lines`在函数级别进行分析的选项。以下是示例输出（[此处为](https://pastebin.com/pFMB3qzg)完整输出）

Line \# Hits % Hits 行内容
========================================
   226 def convert\_to\_notebook(parsed\_json):
   227 结果 = {
   228                             “细胞”：\[\]，
   229                             “元数据”：{}，
   230                             “ nbformat ”：4，
   231                             “ nbformat\_minor ”：2
   第232话
   233                        为 例如 在parsed\_json：
   234 部分 = 示例\[ “部分” \]
   235 build\_up = parts.get（ “ build\_up ”）
   236解释= parts.get（ “解释”）
   237 notebook\_path = “ test.ipynb ”
   238
   第239话
   240 4 2.4 结果\[ “单元格” \] += convert\_to\_cells(build\_up)
   241
   第242话
   243 结果\[ “单元格” \] += convert\_to\_cells（解释）
   244
   245 153 90.0 pprint.pprint（结果，缩进= 2）
   246 1 0.6 使用 open(notebook\_path, " w " ) 作为 f:
   247 12 7.1 json.dump(result, f)

由于它是一个统计分析器并通过以固定时间间隔探测堆栈帧来收集数据，因此它可能不像预期的那样准确，特别是在没有足够重复（或递归）并且程序运行很快的情况下。

[pyinstrument](https://github.com/joerick/pyinstrument)是另一个积极维护的统计分析器，它每 1 毫秒记录一次调用堆栈。pyinstrument 可以测量 CPU 时间，它还支持全栈记录，这意味着它生成的输出类似于（使用[此处的](https://pastebin.com/u8ntLjA2)`--show-all`选项生成的扩展输出：[](https://pastebin.com/u8ntLjA2)

      _     ._   __/__   _ _  _  _ _/_   Recorded: 23:00:06  Samples:  583
     /_//_/// /_\ / //_// / //_'/ //     Duration: 3.023     CPU time: 0.519
    /   _/                      v3.0.3
    
    Program: json_generator.py
    
    3.022 <module>  json_generator.py:11
    ├─ 1.583 pprint  pprint.py:47
    │     [137 frames hidden]  pprint, re
    │        0.606 _pprint_str  pprint.py:244
    └─ 1.421 convert_to_notebook  json_generator.py:226
       ├─ 1.381 pprint  pprint.py:47
       │     [155 frames hidden]  pprint, re
       └─ 0.034 dump  json/__init__.py:120
             [17 frames hidden]  json
    
    To view this report with different options, run:
        pyinstrument --load-prev 2019-06-27T23-00-06 [options]
    

**用法：**

$ pip 安装 pyinstrument
$ python -m pyinstrument some\_file.py

It also provides middlewares for instrumenting [Django](https://scoutapm.com/frameworks/django-monitoring) and flask; you can read more about that [here.](https://github.com/joerick/pyinstrument#profile-a-web-request-in-django)

#### stacksampler, pyflame

Stacksampler is a statistical profiler written in just [100 lines of code](https://github.com/nylas/nylas-perftools/tree/2e9f72ee74587e0dea5ba4826cd60a093c8869f0). It records a stack every 5ms and works well with gevent-based applications. To visualize the results, you can use the complementary [stackcollector](https://github.com/nylas/nylas-perftools) agent, which will generate the flame graphs for you. A flame graph has the call stack at the y-axis and percent resource utilization at the x-axis (CPU time in our case), which enables you to visualize what part of the program is taking what percent of the time. However, due to its simple implementation, there might be some cases (like profiling dynamically spawned processes/threads) where stacksampler won't give the results you want.

Another interesting profiling tool when it comes to generating flame graphs is [pyflame](https://github.com/uber/pyflame) (designed by Uber). The functioning of pyflame is different and more optimized from that of stacksampler (discussed later). The coolest feature of the flame graph is the possibility of calculating a "diff" of two flame graphs. This is immensely helpful when your application starts performing slowly in a recent release. By computing the diff, you can see what changed in the call stack, and how much time it is taking, saving time in debugging performance regressions. However, as per the docs,

> Are BSD / OS X / macOS Supported? Pyflame uses a few Linux-specific interfaces, so, unfortunately, it is the only platform supported right now.

So I wasn't able to test it out and present the output in this post

#### Werkzeug Application profiler

Werkzeug is a library consisting of various utilities for WSGI web applications. [Flask](https://scoutapm.com/frameworks/flask-monitoring) web framework also wraps werkzeug. One utility that werkzeug library provides is Application Profiler middleware. It uses cProfile to profile each request execution. You can see the usage in docs [here.](https://werkzeug.palletsprojects.com/en/0.15.x/middleware/profiler/#module-werkzeug.middleware.profiler)

#### GreenletProfiler

GreenlentProfiler 是一个分析器，适用于分析基于 gevent 的进程。gevent 是一个基于协程的网络库，它在 libev 事件循环之上提供高级同步 API。它用于通过在后台异步执行 I/O 绑定任务（网络调用）来提供并发性（但没有并行性）以提高速度。这是 GreenletProfiler 所有者的[帖子](https://emptysqua.re/blog/greenletprofiler/)，解释了 cProfiler 和 YAPPI 如何对基于 gevent 的应用程序（带有示例）没有太大帮助，而是使用 GreenletProfiler。

#### gprof2dot

gprof2dot 是一种工具，您可以使用它将诸如 cProfiler 和 hotshot（以及许多其他语言分析器）之类的分析器的结果可视化为所谓的[点图](https://en.wikipedia.org/wiki/DOT_(graph_description_language))。

*   使用 cProfile / Hotshot 获取`.dot`文件中的结果
*   有一个用于可视化的 django fontend（称为 graphi）

**用法**

$ pip 安装 gprof2dot
\# 在 osx 上安装 dot 
$ brew install graphviz

$ python -m cProfile -o results.prof somefile.py
$ gprof2dot -n0 -e0 -f pstats -o results.dot results.prof
$ dot -Tpng -o results.png results.dot

这是示例输出（[在此处](/md_blog/public/assets/2021-08-30/results)查看全尺寸图像），它由节点组成

![不明确的](/md_blog/public/assets/2021-08-30/Screen Shot 2019-06-27 at 9.16.21 PM.png)

这里`total time %`表示在函数中花费的时间百分比，`self time %`是不包括子函数调用在函数中花费的时间，`total calls`是总调用次数。

#### pyprof2calltree 和 KCacheGrind

[KCacheGrind](https://kcachegrind.github.io/html/Home.html)是一个非常流行的配置文件数据可视化工具。要使用它可视化您的 Python 程序，您可以使用[pyprof2calltree](https://github.com/pwaller/pyprof2calltree)将 cProfile 结果转换为 callgrind 形式，然后使用 KCacheGrind 将它们可视化。

**用法：**

$ python -m cProfile -o results.prof somefile.py
$ pyprof2calltree -k -i results.prof

这是几个截图，

**![屏幕截图 2019-06-27 at 9.38.14 PM.png](/md_blog/public/assets/2021-08-30/Screen Shot 2019-06-27 at 9.38.14 PM.png)![屏幕截图 2019-06-27 at 9.41.11 PM.png](/md_blog/public/assets/2021-08-30/Screen Shot 2019-06-27 at 9.41.11 PM.png)**

您可以使用 kcachegrind 可视化调用图、调用方图、被调用方图，甚至源代码级别的统计信息。[](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#snakeviz-tuna-and-runsnakerun)

#### snakeviz、金枪鱼和 runnakerun

[sankeviz](https://pypi.org/project/snakeviz/)是一个配置文件可视化工具，可让您将 cProfile 模块的输出转换为基于浏览器的可视化效果，如旭日图和漂亮的图表。[Tuna](https://github.com/nschloe/tuna)（受 SnakeViz 启发）和[runsankerun](http://www.vrplumber.com/programming/runsnakerun/)（该类别中的早期工具）是您可以探索的其他一些选项。

**Snakeviz 用法**

$ pip安装snakeviz
$ python -m cProfile -o results.prof some\_file.py
$ snakeviz results.prof

这是它生成的输出的屏幕截图，

[![Screen-Shot-2019-06-27-at-9-49-43-PM.png](https://camo.githubusercontent.com/22b5cbe25cea1805459b332dccc31df34f374f16/68747470733a2f2f692e706f7374696d672e63632f766d7352687878742f53637265656e2d53686f742d323031392d30362d32372d61742d392d34392d34332d504d2e706e67)](https://postimg.cc/CRm6L1NR)

[![Screen-Shot-2019-06-27-at-9-58-24-PM.png](https://camo.githubusercontent.com/862895a34ee81dbbd3eeb65bbe4cf6c7858b77b9/68747470733a2f2f692e706f7374696d672e63632f36334d4442486a4d2f53637265656e2d53686f742d323031392d30362d32372d61742d392d35382d32342d504d2e706e67)](https://postimg.cc/xJbsRPSm)

CPU 分析器如何在 Python 中工作？
----------------------

尽管您无需了解 CPU 分析器的工作原理即可使用它们，但了解它们的工作方式将有助于您推断其功能和开销。

大多数分析器将作为 Python 进程的一部分运行并访问堆栈帧（其中将包含行号、语句等元信息）。

跟踪分析器通过注册对解释器事件（函数调用、返回、行执行、异常等）的回调来工作。当事件被触发时，回调函数被调用，它具有记录堆栈帧和分析它们的逻辑。用于注册回调的 C 接口函数是`PyEval_SetProfile`和`PyEval_SetTrace`。两者之间的区别在于它们监视的事件。请查看python 文档的[分析和跟踪](https://docs.python.org/3/c-api/init.html#profiling-and-tracing)部分以获取更多详细信息。这是CPython 实现中的[ceval.c](https://github.com/python/cpython/blob/7e1a9aacff95c68d284f31666fe293fa2db5406d/Python/ceval.c)模块（顺便说一下，它是 Python 的核心），其中定义了这些函数。

For checking how they are used, you can see the implementation of the callback functions in the source codes of different tracing profilers. For instance, in the built-in cProfiler, [this](https://github.com/python/cpython/blob/530f506ac91338b55cf2be71b1cdf50cb077512f/Modules/_lsprof.c#L644) is the line of code where callback is registered, and [this](https://github.com/python/cpython/blob/530f506ac91338b55cf2be71b1cdf50cb077512f/Modules/_lsprof.c#L384) is where the callback function is defined.

Coming to sampling profilers, most of them use `setitimer` system call in Linux which asks the OS to send signals after fixed intervals. And just like the callback functions in case of tracing profilers, there are signal handlers which will record the stack frame at that moment for analysis. The "fixed intervals" can be wall time or CPU time, depending on the arguments passed to the system call. Here's documentation for [settimer](https://linux.die.net/man/2/setitimer). You can explore source codes of different profilers and see how they are using it. For instance, [this](https://github.com/nylas/nylas-perftools/blob/2e9f72ee74587e0dea5ba4826cd60a093c8869f0/stacksampler.py#L51) is the line of code is stacksampler where the settimer signal is registered, [this](https://github.com/nylas/nylas-perftools/blob/2e9f72ee74587e0dea5ba4826cd60a093c8869f0/stacksampler.py#L47) is where the signal handler is defined, and [this](https://github.com/nylas/nylas-perftools/blob/2e9f72ee74587e0dea5ba4826cd60a093c8869f0/stacksampler.py#L54) is the implementation of the signal handler.

Now you can also figure out when and why tracing profilers can have high overhead. If the program that you are profiling generates a lot of events (Example, a recursive function), it will cause the callback function to execute at every event, resulting in overhead. The ovehead of sampling profilers will still be roughly the same in these cases. However, one caveat with OS signal based sampling profilers is that the Signals can interrupt system calls (so that signal handler can run). This can lead to IOErrors and may even conflict with other libraries/program that depends on the same signal the profiler is depending on.

That being said, the differentiation between the implementations of tracing and sampling profilers is not very clear-cut. There are also some sampling profilers like Pyinstrument that have shifted to `PyEval_SetProfile` approach, wherein the callback function will not record the stack frames until the "fixed interval" has elapsed (to reduce the overhead). And some sampling profilers (like pyflame) run in a different process altogether making use of [ptrace](http://man7.org/linux/man-pages/man2/ptrace.2.html) system call to record the information for analysis. For those interested, here's a very interseting [blog post](https://eng.uber.com/pyflame/) by Uber engineering on how exactly pyflame works.

现在我们已经了解了分析器的工作原理以及可帮助我们识别瓶颈的所有工具，让我们讨论我们可以遵循的粗略方法来最终优化我们的代码。

[](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#approach-for-optimizing-performance)优化性能的方法
--------------------------------------------------------------------------------------------------------------------

借助分析进行优化就像调试一样。分析器（在大多数情况下）不会告诉您如何更改代码，但它会帮助您验证您的假设。通常，这个过程是这样的；您确定了代码的罪魁祸首部分，然后提出了一些假设

> 以 X 方式而不是 Y 方式执行此操作可能会提高性能。

然后你尝试验证你的假设。如果假设结果为真，您可以使用分析器本身以量化的方式获得性能改进结果。

您可以找出成本高昂的函数（通常“调用次数”和“每次调用的执行时间”的乘积值很高），并探索添加缓存等设计决策（如果函数的输入域是有限的）。您可以更深入地使用线分析器来找出函数内的哪些线最耗时，并尝试查看是否有替代方法。

然而，综合基准是不够的。有时，生产环境和开发环境之间的情况可能会有很大差异。在生产中进行分析很有帮助。然而，在大多数情况下，我们不想承担在生产环境中运行确定性分析器的开销。

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#some-low-hanging-fruits)一些低垂的果实

以下是开发人员在分析 CPU 性能时经常遇到的一些易于修复的想法。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#waiting-time-delays)等待（时间延迟）

这是最明显的唾手可得的果实。任何形式（`time.sleep`、`thread.wait`、`lock.acquire`等）的同步等待都会阻止您的程序在 CPU 上执行任何操作。这个被阻塞的 CPU 时间可以更好地被程序的其他区域利用，这些区域不依赖于等待时间。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#too-much-io-time)太多的 I/O 时间

当您的程序正在等待 I/O 时，它没有使用 CPU。一些示例包括磁盘读取、数据库查询、网络通信（RPC、REST API 调用等）等。

#### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#logging)日志记录

由于记录到磁盘（或屏幕）也是一项 I/O 绑定任务，因此您应该对正在记录的内容和生产中的日志级别保持谨慎。

解决 I/O 绑定任务和时间延迟的方法是异步执行它们（请参阅[asyncio](https://docs.python.org/3/library/asyncio.html) for python3）。

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#way-too-many-function-calls)太多的函数调用

每个函数调用都有开销。在分析时，您可以尝试将昂贵的函数调用归类为

*   真正的来电
*   不必要的调用（算法问题）

这可以使用调用者和被调用者信息来完成；花费的时间信息和配置文件可视化。

在真正调用的情况下，您可能想要探索在较低级别（使用 cTypes、Cython 或 C）实现您的函数或使用已经这样做的库。例如，使用 Numpy 进行矩阵计算要比使用原生 Python 快得多，因为 numpy 是内置的 CPython 并使用 SIMD 架构。类似地，还有像 pandas、scipy、pycrypto、lxml、gstreamer 等库，你可以用它们来做特定的事情，而不是自己做。还有另一个隐藏在这里的通用建议“不要重新发明轮子，首先寻找现有的东西”。

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#object-conversion)对象转换

有一些对象转换任务是 CPU 密集型的，对于您的用例可能不是必需的。例子包括，

*   序列化
*   编码与解码、加密与解密
*   压缩和解压
*   从一个类到另一个类的不必要的转换

### 使用非 Pythonic 构造或过时的方法

在 Python 中有多种方法可以做同样的事情，它们的性能影响可能会有很大不同。以下是一些您应该注意性能影响的实例，

*   字符串连接
*   简单的字符串解析和使用正则表达式
*   在 RAM 中加载大量数据

### 并行化

并行化昂贵的任务也是优化性能的好主意。您可能已经知道，python 中的多线程将提供并发性而不是实际的并行化。为此，您可以使用多处理模块。理想情况下，您最多可以生成核心进程数的 1.5 倍（您可能需要稍微调整此比率以找到应用程序的“最佳位置”）。如果您的程序使用协作多线程（例如，gevent 中的 Greenlets），创建更多进程也会有所帮助。您还可以探索使用 Dask 和 PySpark 等工具的可行性。[](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#some-caveats-must-know-things-about-profilers-and-optimizations)

关于分析器和优化的一些注意事项（必须知道的事情）
------------------------

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#heisenbergs-uncertainty)海森堡的不确定性

海森堡在量子力学中的测不准原理指出，你越准确地知道某物的位置，你就越不能准确地说出它的速度和动量。当涉及到剖析时，可以得出类似的相似之处，即，通过仔细测量一件事，您实际上削弱了对另一件事的理解。你让你的代码以一种方式运行，你尝试真正深入地测量它，将大量钩子放入其中以使用分析器进行这种监控，它实际上可能会改变你的代码的性能。所以最终的数字可能看起来有些不同。即使注入分析器钩子的代码运行速度也不会慢 x%，这 x% 也可能不会均匀分布。因此，如果您想要高精度或正在查看基于百分比的指标，则应牢记这一点。

### 对外部环境的间接依赖

*   分析结果可能取决于外部环境（机器上的平均负载、网络速度等）。因此，为了比较两个程序而进行分析，您应该尽量保持外部条件尽可能相似。

### 受支持的环境有限

某些工具可能支持有限版本的 Python 或仅支持特定的操作系统。

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#hard-to-come-up-with-a-one-fits-all-solution)很难想出一个万能的解决方案

例如，统计分析器不太可能扭曲具有大量函数调用的程序（如 Django 模板渲染器）的时序。这也意味着分析器会丢失有关某些功能帧的记录信息。然而，错过的功能帧是快速运行的。这在生产环境中可能是可以接受的折衷方案，因为开销非常小，并且分析器预计会找到最慢的代码。但是在开发环境中，我们希望对所有事情都有明确的答案，因此我们应该在那里使用确定性分析器。在可视化、报告格式等方面存在类似的权衡。

### [](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#possible-side-effects-of-optimization)优化的可能副作用

这也是优化的一些副作用。

*   编写优化的代码很慢。它需要大量的人力（也需要专家）和时间。示例包括编写 numpy 和 tensorflow 等计算库。
*   优化会损害可读性。优化的代码将提高程序员应该能够理解它的专业知识水平。根据用例的不同，您最终可能不得不使用 C、Cython、CUDA 等低级语言编写代码。
*   优化也可能会影响可扩展性/可维护性。缓存、延迟执行、并行执行和 hacky-patches 等一些优化是基于事物将以某种方式运行的假设。如果这些假设所基于的要求在不久的将来发生变化，则需要更改代码以解决该问题。几个例子可以是，
    *   由于假设的变化，缓存变得无效
    *   在高并发规模的情况下，竞争条件等问题的副作用可能会突然变得明显。
    *   您在设计中为提高速度而进行的时空权衡现在不知何故已成为瓶颈。

### 过度关注分析器

编写完成工作的代码是一回事；以优化的方式编写它是完全不同的游戏。它不仅需要熟悉不同的编程语言结构，还需要深入了解事物的内部运作方式。当然，有一些工具和技巧可以帮助您优化代码。但是相信我，对较低级别发生的事情有一个更好的了解，首先会帮助您编写优化的代码。

APM（应用程序性能监控）工具在哪里以及如何提供帮助？
---------------------------

一个好的 APM 会主动监控应用程序的不同部分，可以帮助您快速定位性能改进的热点。APM 可能不是优化应用程序的灵丹妙药，但它是可以帮助找出瓶颈的主要子弹的集合。例如，Scout 提供：

### 密集的全栈记录

我们主要介绍了专门用于分析 Python 原生的执行速度的工具，但您的应用程序可以不仅仅是在 CPU 上运行的程序。例如，如果您有一个提供良好流量的 Web 应用程序，那么您的堆栈很可能会有组件数据库、外部 API 集成、消费者和生产者等。有时，优化的热点可能在 python 运行时之外。尽管有一些工具可以单独分析这些组件中的每一个，但一个好的 APM 工具将为您提供系统的全局概览和深入挖掘组件的能力。它将在堆栈的不同层记录信息。这对于大型应用程序特别有用。

Scout 研究交易（和跟踪）的概念。因此，从某种意义上说，它是确定性的，每个受监控的事务都被测量。为了收集与不同组件相关的统计信息，scout 使用了诸如[信号和事件](https://github.com/scoutapp/scout_apm_python/blob/master/src/scout_apm/sqlalchemy.py#L29)、[修补](https://github.com/scoutapp/scout_apm_python/blob/master/src/scout_apm/instruments/redis.py#L67)等技术。

### 内存指标

我们没有在这篇文章中介绍特定于 Python 的内存分析器（因为这篇文章已经太长了！），但是 scout 具有以下功能，

*   对象分配跟踪（垃圾收集）
*   能够跟踪 RAM 中的进程内存增加并将它们与特定事务联系起来。

### 一些非常有用的高级功能

您可以将有意义的上下文附加到 Scout 中的事务，以帮助关联和识别特定上下文（用户、操作等）的性能问题。Scout 将每个事务与 Git 版本联系起来，并将直接指向负责的代码行。Scout 可以聚合集群中各种机器的结果。

如果您有一个在 Python Web 框架（如 Django 或 Flask）上运行的 Web 应用程序，您可以通过几行配置直接集成 Flask。

**侦察兵使用**

$ pip install scout-apm

如果你有一个烧瓶、django、瓶子、猎鹰或金字塔应用程序，你可以用几行配置侦察并开始行动。

**在姜戈**

\# settings.py 
INSTALLED\_APPS  \= \[
     " scout\_apm.django " ,   \# 应该首先列出
    \# ... 其他应用 ...
\]
\# Scout settings 
SCOUT\_MONITOR  \=  True 
SCOUT\_KEY  \=  " \[在 SCOUT UI 中可用\] "
 SCOUT\_NAME \=  "一个适合您的应用程序的友好名称。"

**在烧瓶中**

从scout\_apm.flask导入ScoutApm
\# 正常设置烧瓶“应用程序”
\# 将 ScoutApm 附加到 Flask 应用程序
ScoutApm（应用程序）
\# Scout 设置
app.config\[ " SCOUT\_MONITOR " \] \=  True 
app.config\[ " SCOUT\_KEY " \] \=  " \[在 SCOUT UI 中可用\] " 
app.config\[ " SCOUT\_NAME " \] \=  " A FRIENDLY NAME FOR YOUR APP "

Scout 将自动检测与以下库相关的代码，

*   蒙哥
*   要求
*   网址库3
*   Redis
*   弹性搜索
*   Jinja2

要监视其他内容，您可以使用自定义检测。您可以使用`@scout_apm.api.BackgroundTransaction`装饰器或`with scout_apm.api.BackgroundTransaction`指定要监视的代码部分，如果要在该代码中测量特定的工作，您可以使用`@scout_apm.api.instrument()`装饰器或`with scout_apm.api.instrument`上下文管理器，

导入scout\_apm.api
config \= { ' name ' : ' My Test App ' ,
           ' key ' : ' MY\_SCOUT\_KEY ' ,
           ' monitor ' : True }
scout\_apm.api.install( config \= config)
@scout\_apm.api.BackgroundTransaction ( " Some name " )
 def  some\_func ():
     \# some work
    sub\_func()
    \# 还有一些工作
@scout\_apm.api.instrument ( " Sub function " )
 def  sub\_func ():
     \# 一些工作

[![Screen-Shot-2019-06-28-at-12-43-14-AM.png](https://camo.githubusercontent.com/9805f4fe73c1685430885de201994d707c5d8f39/68747470733a2f2f692e706f7374696d672e63632f624e5a6a77686a6b2f53637265656e2d53686f742d323031392d30362d32382d61742d31322d34332d31342d414d2e706e67)](https://postimg.cc/Czp2c95x)[](https://gist.github.com/satwikkansal/e1e6011cb447daac25e0fd5bd295b6b1#conclusion)
===========================================================================================================================================================================================================================================================================================================================================================================================

[![Screen-Shot-2019-06-28-at-12-43-03-AM.png](https://camo.githubusercontent.com/587dac9f31c77c4a8eb4da8cca95ae2d6e3c712d/68747470733a2f2f692e706f7374696d672e63632f717152486e6636662f53637265656e2d53686f742d323031392d30362d32382d61742d31322d34332d30332d414d2e706e67)](https://postimg.cc/wySr8GkF)

结论
--

在这篇文章中，我们通过不同的工具和技术来识别 Python 应用程序中的性能瓶颈，以及优化应用程序性能的一些想法。我们讨论了可用于在 Python 中分析 CPU 性能的不同分析和可视化工具，以及它们的典型用法。我们简要讨论了在尝试优化时应该遵循的方法以及与分析和优化相关的一些注意事项。最后，我们了解了 Scout 的一些功能，这些功能在分析大型应用程序时可能会有所帮助。

================================================================================================

原文:
[使用 Python 和 Pytest 测试并行化](https://python.plainenglish.io/test-parallelization-using-python-and-pytest-2656a4555153)



![简单英语的 Python](/md_blog/public/assets/2021-08-30/1_9f05d96UHcohLvzWQ5mwKw@2x.png)

[](https://python.plainenglish.io/?source=post_page-----2656a4555153--------------------------------)

*   [档案](https://python.plainenglish.io/archive?source=post_page-----2656a4555153--------------------------------)
*   [为我们写信](https://medium.com/javascript-in-plain-english/how-to-write-articles-that-people-want-to-read-6e661edb6d06?source=post_page-----2656a4555153--------------------------------)

使用 Python 和 Pytest 测试并行化
========================

了解如何轻松地并行运行测试以减少 CI 构建时间。
-------------------------

[![乔纳森·汤普森](/md_blog/public/assets/2021-08-30/1_pLHEuQNjZeTSjCfymtUFjQ.jpeg)](https://thompson-jonm.medium.com/?source=post_page-----2656a4555153--------------------------------)

[乔纳森·汤普森](https://thompson-jonm.medium.com/?source=post_page-----2656a4555153--------------------------------)

[跟随](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6aeb61746eb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftest-parallelization-using-python-and-pytest-2656a4555153&user=Jonathan%20Thompson&userId=a6aeb61746eb&source=post_page-a6aeb61746eb----2656a4555153---------------------follow_byline-----------)

[2 月 17 日](https://python.plainenglish.io/test-parallelization-using-python-and-pytest-2656a4555153?source=post_page-----2656a4555153--------------------------------) · 阅读 5 分钟

![](/md_blog/public/assets/2021-08-30/0_hbXQjp6bzH7pfs7M.jpg)

[马丁·桑切斯](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral) ( [Martin Sanchez)](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)上的 [照片](https://unsplash.com?utm_source=medium&utm_medium=referral)

运行自动化 UI 测试可能很耗时，这会导致新构建需要几分钟而不是几秒钟来处理，从而破坏 CI 的稳定性。幸运的是，有一个解决方案， **并行化** 。

测试并行化是在多个处理器之间分配测试负载，然后同时运行所有测试的过程。并行化可以采用多种形式，包括使用单个处理器或设置子进程。结果导致执行时间过短，这将使 CI 构建速度更快。

在本教程中，我们将学习如何使用 Pytest、Pytest-Xdist 和 Playwright 并行运行测试。请记住，这是以适用于使用 Pytest 运行的所有测试的方式编写的。

您无需积极使用 Playwright 即可利用本教程。如果您选择不使用 Playwright，只需使用 Pytest 创建您自己的测试，然后按照以下说明并行运行它们。

入门
==

必须安装以下软件包才能遵循本教程：

*   pytest
*   pytest-xdist
*   剧作家

我强烈推荐这个 `pytest-playwright` 包，因为它为工程师提供了大量方便的装置，可以快速轻松地编写测试。本教程中的测试是使用该 `pytest-playwright` 包编写的 。

运行测试
====

为了了解并行化如何帮助您，我们需要运行一些测试。我已经使用该 `pytest-playwright` 库编写了五个测试 ，并将它们添加为本教程的要点。

将以下要点复制并粘贴到文件名为“test\_elements.py”的 IDE 中。

本教程的示例测试。

We can run the above tests using the command `pytest` in our terminal. Our tests should pass in 16.91 seconds— not too slow for a battery of five UI tests. However, we can speed it up.

![](/md_blog/public/assets/2021-08-30/1_ex7dQcLwKuP0A5ktxUJy8A.png)

Our completed test run without parallelization.

Parallel Lines
==============

Since we know that we have five tests, we can run the tests in parallel on five processors. To do so, we need to input the following to our terminal.

pytest -n 5

Adding “-n” tells the `pytest-xdist` package that you intend to use a specific number of processors. The number after indicates how many processors you would like to use as workers for the run. In our case, we indicate five as we have five tests, therefore we run a single test on a single processor due to `pytest-xdist` load balancing.¹

Load balancing can be further customized if necessary. Engineers can choose to distribute by module and class using `--dist loadscope` or by file with `--dist loadfile` . You may also choose to let the distributor algorithm decide how to load balance using `--dist no` .

This also happens to be the default so an engineer can forego using `--dist no` unless they have to be explicit.

![](/md_blog/public/assets/2021-08-30/1_n48VsrLa4ZtxJNJM9F8nxw.png)

Our completed test run with parallelization.

The image above shows a run time of 8.46 seconds after load balancing on five processors, nearly half the time of the previous run.

Think of the Value
==================

Reducing a test run by seconds does not provide a lot of value. Where parallelization really begins to shine is when you have a large repository of tests which take minutes to run in totality. Parallelizing large runs will see larger gains in overall execution time reduction than small runs.

In my off time I write automation using Python and Playwright in an effort to keep myself sharp and knowledgeable of new automation tooling. I have written a testing repository with 22 tests so far. Running this repository sequentially takes 73.53 seconds.

![](/md_blog/public/assets/2021-08-30/1_J6AfGwbMstd-z2_LvoXEkg.png)

A repository run without parallelization.

When run in CI, it takes my builds an extra minute or so to process due to running UI automation tests. Ideally, we want short build times as they lead to faster response times for build errors and failures.

For the sake of testing, I ran the repository using six processors. The execution time is much more acceptable.

![](/md_blog/public/assets/2021-08-30/1_iUY6fQfc4ChMFCTHY-dC0g.png)

Running the repository on six processors.

A full run of 22 automated UI tests in 16.24 seconds is an asset to a CI build.

For another real world example, I maintained a repository of nearly two-hundred automated tests at one of my previous employers. Running the full repository would take nearly an hour to complete. Instead, I opted to introduce parallelization to cut down on execution time. Parallelized runs using four to six processors would take ten minutes to finish — an overall increase of 600%.

Summary
=======

When running UI tests, you **must** consider parallelization. Not only does it dramatically cut down on execution time, it may also reduce build time if your build process depends on test completion. Faster builds means faster responses to errors or failures, as well as more code deployments.

Resources
=========

1.  “Pytest-Xdist.” _PyPI_ , [pypi.org/project/pytest-xdist/](http://pypi.org/project/pytest-xdist/) .
2.  “Parallel Testing: What It Is and Why You Should Adopt It.” _Bitbar_ , 11 Dec. 2019, [bitbar.com/blog/parallel-testing-what-it-is-and-why-you-should-adopt-it/](http://bitbar.com/blog/parallel-testing-what-it-is-and-why-you-should-adopt-it/) .

**Jonathan Thompson** 是 Pendo.io 的高级质量工程师，专门从事测试自动化。他目前与妻子和一只名叫温斯顿的 Goldendoodle 住在北卡罗来纳州罗利。您可以在 [LinkedIn](https://www.linkedin.com/in/jonathanmnthompson/) 上与他联系 ，或者在 [Twitter](https://twitter.com/jacks_elsewhere) 或 [Github](http://github.com/ThompsonJonM) 上关注他 。

